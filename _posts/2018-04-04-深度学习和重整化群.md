---
layout: post
title: "深度学习和重整化群"
date: 2018-04-04
excerpt: "深度学习与重整化的类比，和近期的相关工作"
tags: [statistics, physics, machine learning]
comments: true
---


* toc
{: toc}

## 引言

本篇需要有一定的理论物理背景才可以更好地理解。
{: .notice}

重整化是高能和凝聚态物理里面最为核心的概念之一。起初重整化作为消去高能场论中， QED 圈图计算时出现的无穷大的技巧被引入，这一技巧很快蔓延到了量子场论的各个角落。只要需要计算圈图，人们就不得不面临着物理量无穷大的发散，这一发散不得不用所谓的重整化手续来扣除。神奇的是，通过在场论计算里使用这一稍显随意的扣除无穷大的数学技巧，人们得到的对真实物理量，比如耦合常数，散射截面的理论计算值，和实验符合的惊人的好。虽然如此，还是有很多老牌的物理学家拒绝相信这一莫名其妙的数学游戏，就如同爱因斯坦一定程度上拒绝了量子力学的概率阐述一样。物理学家在之后的几十年里，慢慢试图理解所谓的重整化背后的物理究竟是什么。

现在，我们对于重整化背后的物理内涵，大概有了一些认识。特别是从凝聚态物理的视角，Wilson 重整化群（renormalization group）的思想给出了一个更清晰的物理图像。简单的说，我们可以定义一种操作来减少系统的自由度，而保持系统里的某些量不变，反复进行这种操作，系统的自由度就会越来越少，而新的这些较少的自由度构成的有效理论和原来的理论在某种程度上相同。这些减少自由度的操作就构成了一个群（传统意义上是个不可逆的半群）。这些减少自由度的操作往往和能量标度的降低联系在一起，操作连续作用就形成了重整化流，而通过系统有效理论的变化，我们还可以分析出系统的标度性质。

通过重整化群的方法，我们可以发现大量微观上完全不同的系统，随着重整化过程，都会流到（或流出）同样的不动点，具有相同的标度特征，我们称这些不同的系统处于同一个普适类。在某种意义上，虽然这些系统微观模型千差万别，但它们低能都对应同样的物理。



## 经典场论的重整化形式体系

所谓场论（我们这里只讨论虚时意义上的经典统计场论，量子场论可以映射为高一维的经典场论，实时间可以 Wick rotation 成虚时），就是给出了一个关于若干（可以是无穷）自由度的一个分布族，这一族分布具有相似的形式，区别只是参数不同，这里的参数就是物理上的温度。

具体地说，原始的系统具有自由度 $$X=\{x_i\}, i=1,2...N$$，为了简化问题，假设每个自由度只可以取两个值$$x_i=\pm 1$$。所谓场论就是给出一个哈密顿量 （依赖于自由度$$X$$的函数）：$$H(X)$$。那么对于自由度的每一个构型，其出现的概率就定义为$$P(X)\propto e^{-\beta H(X)}$$。其中$$\beta\propto\frac{1}{T}$$描述了分布随着温度的变化。我们可以定义概率的归一化因子 $$Z=\sum_X e^{-\beta H(X)}$$，这样我们就可以直接给出每一种构型出现的概率值$$P(X)=\frac{e^{-\beta H(X)}}{Z}$$。$$Z$$ 在物理上被称为配分函数，可以证明物理上很多重要的物理量都可以由配分函数给出，比如系统的自由能为$$F\propto -T\ln Z$$。配分函数的作用远超过了一个归一化因子的地位，一旦我们求出了配分函数，我们就知道了该系统的几乎所有物理量。统计力学和场论中的配分函数扮演的角色等同于量子力学中的波函数。

问题就在于，待求的配分函数涉及对所有构型$$X$$的求和，这一求和共有$$2^N$$项，这一计算的复杂度是不可接受的。一种解决方案就是数值上的蒙特卡洛方法，这里通过随机取样，接受率的表达式只和两个构型概率之比有关，因此不需要知道配分函数便可以进行模拟。另一种解决方案就是所谓的重整化，通过减少自由度，并且**保持配分函数不变**，我们可以得到一个自由度较小的系统$$L=\{l_i\}, i=1,2...N_L$$和对应的有效场论$$H'(L)$$，通过这一等效理论，我们可以估计配分函数。如果反复的减少自由度，我们甚至还可以从这一过程中获取更多有趣的信息，比如普适类和标度特征等。事实上重整化工具分析临界指数等标度特征和相变非常有力，以至于很少有人再去关心配分函数的事了。

具体的，我们假设对于新系统的构型概率分布：(我们之后的讨论将温度参数$$\beta$$ 吸收进哈密顿量中)

$$
P(L)\propto\sum_X e^{T(X,L)-H(X)}
$$

也即我们变相的通过在原始哈密顿量中添加了旧自由度$$X$$和新自由度$$L$$的耦合项作为新的哈密顿量来刻画系统的概率分布，并通过对旧自由度求迹来得到关于新自由度的概率分布。此时的配分函数为

$$
Z'=\sum_L \sum_X e^{T(X,L)-H(X)}
$$

我们要求新系统的配分函数和旧系统相等$$Z'=Z$$，来保证重整化操作两个系统的“物理本质”不变，由此可以得出关于耦合项$$T$$的约束：

$$
\sum_L e^{T(X,L)}=1
$$

我们发现如果令$$P(L\vert X)=e^{T(X,L)}$$，以上归一化自动满足。也就是说，我只要任意给定一个随机变量 $$L\vert X$$，也就是给出一个新自由度关于旧自由度的条件概率分布，我们就得到了一种严格，良好定义的重整化方案。如果我们假设新变量的独立性，即$$P(L\vert X)=\Pi_i P(L_i\vert X)$$，那么问题就得到了进一步的简化。此时我们可以得出新系统的等效哈密顿量为

$$
H'=-\ln \sum_X P(L\vert X)e^{-H(X)}
$$

举个例子，如果原始系统的哈密顿量为

$$
H=\sum_iJ x_ix_{i+1}
$$

如果我们通过选取了某种重整化方案，也即某个条件概率$$P(L\vert X)$$的函数形式，得出了新系统的哈密顿量为

$$
H'=\sum_i J' l_il_{i+1}
$$

随着我们迭代进行重整化操作，系统自由度不断减小，哈密顿量的参数不断变化：$$J\rightarrow J'\rightarrow J''\rightarrow …$$，这一参数的变化就好像是在参数空间的流动一样，这就被称为重整化流。重整化过程反复进行，会使得参数趋于某一个定值（无穷大也可理解为一个定值），这就是该系统的不动点。

如果有另一个原始系统，其哈密顿量为

$$
H_2=\sum_i J x_ix_{i+1}+\sum_i K x_ix_{i+2}
$$

经过相同的重整化方案不断操作，你会发现系数$$K$$会流到$$K=0$$，最后得到的有效哈密顿量和$$H$$的等效理论相同，这种情况我们称$$K$$对应的次近邻耦合项是 irrelevant。也就是说虽然两个系统的微观细节不同（是否有次近邻相互作用），但在重整化的意义下，有效场论都是一样的，这被称为两个系统处于同一个普适类（不太严格，普适类通常描述的是另一种类型的不动点）。更进一步的，我们也可以在参数的流动中，提取出一些临界行为的信息，这一类信息，是同一普适类的系统所共享的。对于物理系统，这些标度信息可以在相变相关的实验中测量到，使得重整化这种方法有了可以预言的物理行为。而这一重整化的思想和精神，也几乎是凝聚态物理的基石。

## 限制波尔兹曼机

上面提出的关于经典统计场论的重整化问题，归结于给定一种 $$L$$ 关于$$X$$ 的条件概率的问题。机器学习中的 RBM 模型早就被证明可以用来拟合任意分布，那么我们只需要构造一个可见层为$$L$$，隐藏层为$$X$$的RBM，我们就可以来固定下这一条件概率。

具体的对于RBM，我们也可以定义一个能量：

$$
E_\Lambda(X,L)=\sum_{ij}-x_i \lambda_{ij} l_j-\sum_i a_i x_i-\sum_j b_j l_j
$$

其中 $$\Lambda=\{\lambda_{ij},a_i,b_j\}$$ 代表了RBM的参数集。那么对于新旧自由度的联合分布，我们有

$$
P(X,L)\propto e^{-E_\Lambda (X,L)}
$$

而对于重整化需要的条件分布，我们则有

$$
P(L\vert X)=\frac{P(X,L)}{P(X)}=\frac{e^{-E_\Lambda (X,L)}}{\sum_L e^{-E_\Lambda (X,L)}}
$$

一切都看似简洁完美，那么问题来了： 1）这个RBM该怎么训练？ 2）参数集$$\Lambda$$该怎么取？ 3) 既然随便一个条件概率 $$P(L\vert X)$$都对应一种良好定义的重整化方案，那么我们究竟该选哪个条件概率？可以看出这三个问题实际上都是同一个问题，归结起来就是无穷多可以成立的重整化方案我要选哪个？这背后还隐含了一个更深刻的问题，不同的重整化方案给出不动点，重整化流和临界指数相同吗？这些问题的思考，我将在下面相关工作的叙述和反思中稍作展开。

### 一些相关工作

在 **arXiv: 1410.3831** 这篇工作中，作者给出了一个想当然的 RBM 构造。既然 RBM 以前都是通过最小化模拟的分布和数据集的分布的 KL 散度来训练的，也就是利用所谓的 contrastive divergence 算法，那么我们还是这么来训练这个 $$\Lambda-RBM$$。首先用蒙卡按概率采样，生成一些物理上的自由度构型$$X$$，然后在这些数据上按老办法直接训练 RBM，使得$$P(X)$$最大即可。训练完第一层 RBM 之后，我们可以将隐藏层的概率取样作为新的输入，继续训练下去，从而得到一个堆叠起来的深度RBM（深度信仰网络），这就类比了重整化群的迭代操作。

这种方案虽然训练 RBM 的过程似曾相识，不过没有什么明显的意义。我们这里的 $$\Lambda-RBM$$ 是用来给出重整化方案的，按前边所说，任意参数的 RBM 都可以给出良好定义的重整化方案，这套训练 RBM 的老办法只是挑出了一种重整化方案，而不是实现了一种重整化方案。但作者并没有说明为何使用最小化KL散度的目标来选出特定的重整化方案。作者用这种RBM在一维和二维Ising模型上进行了训练，不过只看了不同层自由度的所谓“接受场”，也即和前一层非零权重的连接有多大范围（之所以RBM层间大部分权重是0，是因为作者训练时使用了$$L_1$$正则项）。

**arXiv:1801.07172** 这篇工作则另辟蹊径，作者只训练一个RBM就实现了重整化流。RBM的训练过程还是传统的CD算法最大化数据集可能性，只不过之后就一直通过该RBM来迭代生成新的自由度构型（自由度数目并不减少！），生成方法和CD训练类似，给定可见层，生成隐藏层均值，在回过头生成新的可见层均值，并按照均值概率取样成新的可见层自由度构型。如此迭代下去，就可以得到系统的重整化流。通过测量每一个构型对应的温度（由一个监督学习的简单全连接网络实现），就一定意义上实现了一个重整化流（温度或者说耦合参数的跑动）。

这一构造看起来和我们理解的重整化过程似乎有点区别，不论怎样，文章的结果看起来挺吸引人。首先RBM的训练数据，就是不同温度的构型混在一起的，如果温度区间跨越了2D Ising 模型的临界温度，观察到的重整化流显示低温和高温构型都会流向临界温度附近的构型，这和传统的重整化群能标的跑动方向恰好相反！在传统的重整化中，随着高能自由度逐渐被积掉，我们可以观察到临界点附近的等效场论流向了零温和无穷高温度的不动点。为了验证，作者又使用了临界温度附近的构型作为迭代起点，则发现温度对应的重整化流不怎么动，还固定在临界温度附近。不过这一结论也不完美，作者发现当使用的 RBM 隐藏层神经元数目比可见层多时，重复以上过程得到的重整化流的行为发生了改变：无论初始构型来自什么温度，都会重整化流到高温不动点。另一方面，如果迭代用的 RBM 仅由高温区或低温区的构型训练得出的话，重整化流则会只流到对应的不动点。

作者觉得这一 RBM 可以在没有先验知识的情况下就能找到临界温度还是很惊人的（但实际上，PCA 就足以找到二维 Ising 模型的临界温度了）。作者的解释是这个 RBM 学到了所有温度的特征，而临界温度的构型恰好包含所有温度的特征（原文如此）。一个证据就是，如果训练集里去掉临界温度的构型，得到的 RBM 还是可以产生流到临界温度的重整化流。至于隐藏层神经元多时找不到相变点的行为，作者归咎于过于复杂的 RBM 学到了过多 irrelevant 的特征。证据则是 RBM 权重矩阵 $$WW^T$$ 的矩阵元（和自由度之间的关联函数有关）分布的比较。

该工作的构型迭代过程和重整化的框架似乎不怎么相符，这一迭代导致对应温度构型流到低温高温或临界温度的不动点是完全有可能的，但似乎和重整化并没有什么紧密的关系，也看不出构型迭代对应的能标跑动在哪里。一定程度上，2D Ising 模型就像是机器学习里的 MNIST 数据集，来一只狗，随便搞个阿猫算法，都能说的头头是道，换个模型，多一半套路就现原形了。因此我对这篇进一步探索 RBM 和 RG 关系的文章持谨慎看法。里面很多关于 RBM 学到了哪些信息的分析还是很精彩的，但这些操作本身实在不怎么对应的重整化过程。

### 监督学习 RBM

正如上面所说，似乎没人给出什么道理来说明为何要用一个 CD-k 训练拟合可见层分布的 RBM 来做重整化方案。**arXiv: 1608.08225** 就做出了批评，并指出 RG 本身是一个监督学习的过程，你想留下什么特征取决于你最后想要什么。比如在物理问题里，大家希望留下低能宏观的特征。不同的 RG 方案，也即不同的 $$P(L\vert X)$$对应留下的特征不同，不进行指定，是无法无监督学习得到一个适合研究问题的 RBM， 也即重整化方案的。换句话说，粗粒化加保持配分函数不变，无法完整的定义一个重整化。

不过很快 arXiv: 1410.3831 的作者就写下了 **arXiv:1609.03541** 做出了反驳。不过这一反驳虽然指出了上文一个反例构造的不足，但并不足以 justify 重整化方案的唯一性。事实上，重整化方案就是不唯一的，需要靠具体问题和期望的特征进行选择。

基于上面的思考，就有了 **arXiv: 1704.06279** 这篇文章，重新定义了 RBM 的训练方式，由此期望得到更有意义的重整化方案。这篇文章将 RBM 的训练目标从最小化 KL 散度 $$D(P_\Lambda(X)\vert P(X))$$ 换为了最大化交叉信息 (mutual information) 

$$
I_\Lambda(L:E)=\sum_{L,E}P_\Lambda(E,L)\ln \frac{P_\Lambda (E,L)}{P_\Lambda(L)P(E)}
$$

这里的$$L$$ 是新自由度，也即 RBM 隐藏层的自由度。而对于本来物理系统的自由度处理则稍显复杂，需要将 $$X$$ 分成待重整块（也即 RBM 的可见层自由度）$$V$$，环境自由度 $$E$$（实际操作上，还需要将$$V$$和$$E$$分隔开的缓冲区$$B$$，不过概念理解上不需要）。

我们注意到这样训练出来的 RBM 可见层和隐藏层都很小，实践上对于二维模型，通常 $$V$$ 只取四个自由度，而 $$H$$ 只取一个自由度。比起前文作者往往训练一个可见层和系统一样大的整体的 RBM，这里的作者默认了，整个系统的 RBM 可以由这种训练出的小 RBM 平铺构成。换句话说，如果将整个系统视为一个 RBM，则每个隐藏层神经元只和极少的可见层神经元有非零连接，整个 RBM 具有一定的局域性，这也和 arXiv: 1410.3831 的工作结果相符(利用$$L_1$$正则惩罚项)。这种局域连接的 RBM 甚至被物理学家起了个名字叫： further restricted RBM。名字的出处详见 arXiv: 1609.09060，这种结构在该文被用来精确表示 toric code 模型的波函数。

书归正传，通过新的损失函数和训练方式，作者得到了和无监督学习不同的 RBM，从而扫描原系统构型来生成自由度更小的新构型。之后我们基于新的构型可以训练出新的 RBM，如此迭代，依旧是用神经网络的深度来模仿重整化群的步数或者说能量标度。对于每次通过 RBM 生成的更小的新系统，我们进行关联函数计算来估计新系统的等效温度。注意到这并不要求我们有，关联函数和温度依赖关系的先验知识；而可以提前从不同温度蒙卡样本和对应的关联函数中拟合出来（作者所谓的 thermometer）。通过这种方法，可以计算了温度参数的重整化流。依旧是对于 2D Ising 模型，这次的重整化流和物理上的完全相同。由此我们可以定位出相变点的位置，从而有 finite size scaling

$$
\frac{\beta_*(l,L)-\beta_c}{\beta-\beta_c}=f((\frac{L}{l})^{1/\nu})
$$

其中，$$l$$是现在系统的尺度，而$$L$$是原系统的尺度。$$\beta$$为初始温度，$$\beta_c$$为临界温度，$$\beta_*$$则为重整化构型的等效温度。通过将相关数据点进行 data collapse，就可以得到临界指数 $$\nu\approx 1$$（解析结果为1）。不过原文这一估计误差很大——对临界指数的估计总是误差很大。

了解了这种方法分析重整化流的定量方法之后，我们再回到 RBM 构建的探讨。为何选择交叉信息作为损失函数训练 RBM，当我们讨论重整化方案选择时，我们究竟在讨论什么？作者认为，有效的重整化方案除了满足自由度减少和保持配分函数不变之外，还需要满足其他一些条件，才构成了物理上理解的 RG。这些条件主要指相应 RG 方案应该使得生成的等效哈密顿量保持局域性，等效哈密顿量应该尽可能只包括少体和短程的相互作用。换句话说，我们希望重整化的方向平行于特征能量降低的方向，这和物理直觉上的 RG 才相符。而其他不同方案的 RG 可能会给出很多奇怪的结果。至于为何最大化交叉信息可以实现上述的和物理符合的 RG 方案，作者并没做出解析上的严格推导，仅用了1维 Ising 模型的例子进行了计算分析，得出结论最大化交叉信息和物理上最优的 RG 方案相符。直观上理解，最大化隐藏层和环境的交叉信息，等价于使得新变量耦合到和环境关联最强的可见层自由度上，这一直觉和所谓的 relevant operator 相符，因此可以在粗粒化的过程中选出 relevant 场自由度。

这一文章还有个亮点，除了二维 Ising 模型，还稍微考虑了经典 dimer 模型，通过 RBM 权重说明这种重整化可以提取相对复杂模型的 relevant 自由度，比 Ising 模型的孤例靠谱些。另一方面作者也比较了自己的训练方法和原始无监督学习方式的 RBM，在加噪声的情况下，原始 RBM 就无法很好的提取出 relevant 自由度了，而基于交叉信息的 RBM 依旧工作良好。这篇工作还有很多细节值得回味，特别是如何解析证明 RBM 这种训练方式合理性，值得进一步思考。而几个可能的问题是，这种 RBM 做 RG 的工作流，还是无法提取 non-local 的 relevant 自由度，所谓 dimer 模型里自由度提取还是局域的，将对应的 RBM “铺满”好像不能得出真实的物理自由度。换句话说，dimer 模型似乎无法很好的用上面的工作流来定量计算重整化流。所有上面的工作似乎还没有完美的解决 RBM 类型的重整化分析的问题：无论是理论上还是数值上。

### 一点思考

看完了以上工作，我们在回过头来回答最初几个关于重整化的问题。重整化方案有无穷多，一个物理上的重整化方案一定还包括了粗粒化，配分函数不变以外的特征。首先应有的特征就是参数流动是连续函数。考虑热力学极限（无穷自由度）的系统的重整化，粗粒化可以几乎连续的实现，在这种情形下，对应的参数跑动需要满足连续性（应该需要满足更强的可微性），这也是重整化流有意义的基础。一个反例则是，在传统的 block spin 重整化方案的每一步最后，再把新的自旋自由度打乱（注意这依旧是良好定义的严格 RG）。这样每一步 RG 都会有一些新的耦合参数出现，另一些参数消失，这样的重整化明显无法带来有意义的重整化流，不连续的函数也无法定义临界指数。

第二个特征就是，一个好的 RG 方案，应该使得不动点有效哈密顿量中的参数尽可能的少，这一要求比较强，是否真的必要也有待进一步思考。也就是说，不满足这一要求的若干重整化方案，是否能给出背后物理实质相同的重整化流图和临界行为？如果可以，那么这个要求就没什么特别的意义。

最后就是，如果我们添加了以上两个特征作为重整化方案的选择条件，那么对应到 RBM 的训练，究竟应该选择何种损失函数，交叉信息是否是一个最优的选择？亦或者针对不同模型我们需要用不同的方式训练 RBM？满足这两个条件的重整化方案又是否在某种意义上唯一？这些问题也没有决定性的答案出现。



## 其他和重整化群的类比

首先要说明这些类比的哲学，正如 arXiv: 1608.08225 所言：
> Calling some procedure renormalization or not is ultimately a matter of semantics; what remains to be seen is whether or not semantics has teeth, namely, whether the intuition about fixed points of the renormalization group flow can provide concrete insight into machine learning algorithms.

也就是说，机器学习很多模型，甚至现实生活的很多问题，都可以和重整化群进行类比，so what？如果这一类比，没给手头的问题带来新的定量的思考方式和解决问题的方法，就仅仅停留在简单的相似上。

如果我们满足找到这种可能没什么意义的相似，那机器学习里有很多例子，比如卷积神经网络就很像实空间的重整化，PCA或是autoencoder这种信息压缩和降维的工具，也可以在某种意义上理解成重整化。（CNN 和 RG 像到什么程度呢，几乎每一个第一次接触 CNN 的物理学家，都会感慨一句：这不就是 real space RG 嘛。）关于 PCA 和 RG 的类比，可以参考 **arXiv: 1610.09733**。而关于 MERA （计算物理中一种数值变分重整化的结构）和神经网络的类比，可以参考 **arXiv: 1301.3124**。至于更多的利用 tensor network 来代替神经网络做机器学习的工作，我认为已经和本文的 RG 关系不大了。

这里我们着重介绍下 **arXiv: 1802.02840** 的工作，这篇文章没有使用 RBM 这一框架，而是利用机器学习中另一个著名的工具 Normalizing Flows 来实现了 RG 的过程。所谓的 Normarlizing flows 其实就是函数作用在随机变量上来改变分布的过程，多层函数连续作用在随机变量上可以实现不同的分布，因此这一生成模型和 RBM 颇有相似之处，也就难怪其可以在重整化这一问题上有用武之地。

这篇工作通过构造深层的 Nomarlizing flow （以 Real NVP 来实现），来实现重整化过程（逆向看每层忽略一半自由度，正向看每层注入等量的高斯噪声）。由于 nomarlizing flow 是双向的，这里的重整化群真的实现了一个群结构而不只是半群。该模型的前向，输入多元高斯分布的采样就可以输出对应物理模型的构型。而反向训练过程采用的损失函数则为

$$
L=\int d\mathbf{x}\; q(\mathbf{x})[\ln q(\mathbf{x})-\ln \pi(\mathbf{x})]
$$

其中 q 为输出的概率分布，可由 real NVP 计算，通过原始的高斯分布得出。而 $$\pi(x)=e^{-\beta H(x)}$$ 和对应物理系统构型的概率成比例。考虑到

$$
L+\ln Z=KL \left(q(\mathbf{x})\vert \frac{\pi(\mathbf{x})}{Z}\right)\geq 0
$$

通过训练该模型，在使得输出分布更接近物理分布的同时，我们还顺便用损失函数的下界估计出了自由能上界（有点类似 VAE 目标函数的一石二鸟）。实现了这一个双向 RG 模型的好处是，除了可以逐层粗粒化提取物理系统（或是 MNIST 数据集）的低能信息；我们可以反过来，通过高斯取样得出物理的构型，由此将该模型视为一个蒙卡取样器，用来计算其他物理量的期望值。不过作者并没有进一步通过不同深度的构型变化来估算温度的重整化流，虽然原则上应该可以做。

重整化和神经网络的结合才刚刚开始，这一领域还有很多亟待解决的问题，也在呼唤更多优秀的工作。这一结合的深入无疑将带给理论物理和机器学习两个领域更多新鲜的想法和活力。




## 参考文献

详见文内高亮的paper，如果有条件，请尽量阅读对应工作的正式发表版本，有些工作较预印本变动较大。

EOF