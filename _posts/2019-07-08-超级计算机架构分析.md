---
layout: post
title: "超级计算机架构分析"
date: 2019-07-08
excerpt: "以著名超算为实例，看 HPC 集群的体系结构"
tags: [hpc, hardware, linux]
comments: true
---

* toc
{:toc}

以下内容都基于可公开获取的资料，和普通的超算用户权限可获取的信息。如有不适宜公开的信息和数据，可以联系我删除。本文仅作学习探讨，请勿利用本篇的信息和知识，从事任何破坏计算机系统的活动。
{: .notice}

## 概述

一个大规模的超算，按照传统的 HPC style，根据功能不同，大概粗略分为几种不同的节点。

* 登陆节点

  和普通用户交互的主要节点。主要用来接受用户的 ssh 连接和文件传送，同时可以用来编译程序，修改代码和通过任务管理系统提交任务到计算节点。登陆节点还可细分为最外侧的负责 VPN 对接外网和入流量分流负载均衡的节点，以及普通的用来浏览文件编译程序提交任务的节点，和负责高带宽大文件传输的专用节点等。

* 控制节点

  普通用户无法登陆，一般具有单独的管理网络，用于管理软件部署的节点。还可细分为提供资源管理软件，提供账户管理和网络服务等基础设施，提供数据库后端，提供监控软件后端，提供各种控制面板前端等不同功能的节点。分类越细，高可用就越好。每一种功能，还可以进一步包括主节点和备用从节点，从而防止单点故障。

* 存储节点

  通过网络文件系统，如 Lustre，共享给登陆节点和计算节点使用。通常与这些节点通过高速互联网络，比如 InfiniBand 相连接，带给用户调用本地文件的速度。还可细分为存储元数据的节点，存储文件内容的节点，备份数据的节点等。

* 计算节点

  用来进行计算任务的节点，占据了超算节点中的绝大多数。还可细分为不同硬件特性的计算节点。比如大内存节点用来解决内存瓶颈的问题，现在最大内存可达 3T。又比如多 GPU 节点，用来进行机器学习等任务。还有具有本地固态硬盘的节点，用来满足需要高速 IO 的计算任务的需求等等。

对于小型的 HPC 集群，很多时候往往登陆节点，控制节点和存储节点的角色，都由同一台机器担任，同时整个系统也往往只具有一套网络。

本文将结合国内规模最大的超级计算机之一，来探讨前述的节点架构和一些软硬件的细节。

## 硬件与软件

### 硬件相关

```bash
$ hostname
cn0
$ lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                64
On-line CPU(s) list:   0-63
Thread(s) per core:    2
Core(s) per socket:    8
Socket(s):             4
NUMA node(s):          4
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 45
Stepping:              7
CPU MHz:               1200.000
BogoMIPS:              4799.37
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              20480K
NUMA node0 CPU(s):     0-7,32-39
NUMA node1 CPU(s):     8-15,40-47
NUMA node2 CPU(s):     16-23,48-55
NUMA node3 CPU(s):     24-31,56-63
$ cat /proc/cpuinfo|grep "model name"|head -n1
model name	: Intel(R) Xeon(R) CPU E5-4640 0 @ 2.40GHz
$ cat /proc/cpuinfo|grep "siblings"|sort -u
siblings	: 16
$ numactl -H
available: 4 nodes (0-3)
node 0 cpus: 0 1 2 3 4 5 6 7 32 33 34 35 36 37 38 39
node 0 size: 32741 MB
node 0 free: 7251 MB
node 1 cpus: 8 9 10 11 12 13 14 15 40 41 42 43 44 45 46 47
node 1 size: 32768 MB
node 1 free: 11946 MB
node 2 cpus: 16 17 18 19 20 21 22 23 48 49 50 51 52 53 54 55
node 2 size: 32768 MB
node 2 free: 13 MB
node 3 cpus: 24 25 26 27 28 29 30 31 56 57 58 59 60 61 62 63
node 3 size: 32768 MB
node 3 free: 17173 MB
node distances:
node   0   1   2   3
  0:  10  20  20  20
  1:  20  10  20  20
  2:  20  20  10  20
  3:  20  20  20  10
$ cat /proc/meminfo |grep Total
MemTotal:       256244660 kB
SwapTotal:      131071992 kB
$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda      8:0    0  1.1T  0 disk
|-sda1   8:1    0  9.8G  0 part /boot
|-sda2   8:2    0  125G  0 part [SWAP]
`-sda3   8:3    0  981G  0 part /
sr0     11:0    1 1024M  0 rom
```

登陆节点上是 4 路 CPU，安装的是 E5-4640。每个 CPU 物理核心是 8 个，因此开启了超线程。登陆节点配备了 256G 内存和 130 G 的交换内存。登陆节点本身有一块 10T 的本地硬盘，除了交换内存分区外，基本都挂载了根路径上。`sensors` 可以查看登陆节点的温度，大概平均在 45 度左右。

```bash
$ hostname
cn9886
$ cat /proc/cpuinfo|grep "model name"|head -n1
model name	: Intel(R) Xeon(R) CPU E5-2692 v2 @ 2.20GHz
$ cat /proc/cpuinfo | grep "physical id" | sort -u|wc -l
2
$ cat /proc/cpuinfo | grep "core id" |wc -l
24
$ cat /proc/cpuinfo|grep "siblings"|sort -u
siblings	: 12
$ cat /proc/meminfo |grep Total
MemTotal:       66085420 kB
SwapTotal:             0 kB
$ ls /dev|grep "sd\|hd"|wc -l
0
```

由以上信息，可以看出对于普通计算节点，安装有双路二代 E5 CPU，每个 CPU 12 核，超线程并未启用 （siblings 和物理核心数相等）。内存为 64G，没有交换内存。计算节点应该是完全没有本地硬盘的，操作系统挂载在内存文件系统中，可参考文件系统部分。考虑到计算节点的多态性，以上硬件指标只针对了最普通的计算节点。

### 操作系统

操作系统相关，该超算基于 Red Hat 6.5 和 Linux 2.6.32 内核，登陆节点和计算节点相同。

```bash
$ uname -r
2.6.32-431.TH.x86_64
$ lsb_release -a
LSB Version:	:base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
Distributor ID:	RedHatEnterpriseServer
Description:	Red Hat Enterprise Linux Server release 6.5 (Santiago)
Release:	6.5
Codename:	Santiago
$ cat /etc/system-release
Red Hat Enterprise Linux Server release 6.5 (Santiago)
$ cat /proc/version
Linux version 2.6.32-431.TH.x86_64 (root@ln1) (gcc version 4.4.7 20120313 (Red Hat 4.4.7-4) (GCC) ) #66 SMP Sat Jan 10 13:54:00 CST 2015
```


### 文件系统

```bash
$ hostname
ln0
$ df -HT
Filesystem                                Type    Size  Used Avail Use% Mounted on
/dev/sda3                                 ext4    1.1T  110G  875G  12% /
tmpfs                                     tmpfs   132G  2.6M  132G   1% /dev/shm
/dev/sda1                                 ext4     11G  206M  9.6G   3% /boot
89.72.14.*@o2ib:89.72.14.*@o2ib:/WORK lustre  3.3P  2.6P  561T  83% /WORK
89.72.14.*@o2ib:89.72.14.*@o2ib:/HOME lustre  429T  248T  160T  61% /HOME
89.72.14.*@o2ib:89.72.14.*@o2ib:/CLS  lustre  694T  460T  200T  70% /CLS
$ hostname
cn9886
$ df -HT
Filesystem                                Type    Size  Used Avail Use% Mounted on
none                                      tmpfs    34G  965M   33G   3% /
89.72.14.*@o2ib:89.72.14.*@o2ib:/WORK lustre  3.3P  2.6P  561T  83% /WORK
89.72.14.*@o2ib:89.72.14.*@o2ib:/HOME lustre  429T  248T  160T  61% /HOME
```

可以看出，计算节点是无盘安装的，因为根路径挂载在 tmpfs 这种内存文件系统之上。文件系统的主要部分 /WORK，由专门的大量存储服务器集中提供 Lustre 文件系统供所有节点使用。

### 网络设置

```bash
$ hostname
ln0
$ ifconfig
eth0      Link encap:Ethernet  HWaddr 74:A4:B5:00:00:54
          inet addr:172.16.22.*  Bcast:172.16.22.255  Mask:255.255.255.0
          inet6 addr: fe80::76a4:b5ff:fe00:54/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:59969052877 errors:0 dropped:790 overruns:5891 frame:0
          TX packets:57357017860 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:48611645779968 (44.2 TiB)  TX bytes:71213580443888 (64.7 TiB)
          Memory:c7d60000-c7d80000

eth1      Link encap:Ethernet  HWaddr 74:A4:B5:00:00:55
          inet addr:25.25.107.*  Bcast:25.25.107.255  Mask:255.255.255.0
          UP BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:0 (0.0 b)  TX bytes:0 (0.0 b)
          Memory:c7d40000-c7d60000

gn0       Link encap:Ethernet  HWaddr FC:FC:FC:02:44:0B
          inet addr:12.11.70.*  Bcast:12.255.255.255  Mask:255.0.0.0
          inet6 addr: fe80::fefc:fcff:fe02:440b/64 Scope:Link
          UP BROADCAST RUNNING  MTU:32750  Metric:1
          RX packets:2675487957 errors:0 dropped:1 overruns:0 frame:0
          TX packets:3877092191 errors:0 dropped:54 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:2256180513495 (2.0 TiB)  TX bytes:19671889758943 (17.8 TiB)

Ifconfig uses the ioctl access method to get the full address information, which limits hardware addresses to 8 bytes.
Because Infiniband address has 20 bytes, only the first 8 bytes are displayed correctly.
Ifconfig is obsolete! For replacement check ip.
ib0       Link encap:InfiniBand  HWaddr 80:00:00:48:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00
          inet addr:89.72.2.*  Bcast:89.72.255.255  Mask:255.255.0.0
          inet6 addr: fe80::202:c903:2c:cac1/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:2044  Metric:1
          RX packets:35302692014 errors:0 dropped:0 overruns:0 frame:0
          TX packets:10946201 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:256
          RX bytes:1978376162048 (1.7 TiB)  TX bytes:1164327013 (1.0 GiB)

virbr0    Link encap:Ethernet  HWaddr 52:54:00:91:5B:20
          inet addr:192.168.122.1  Bcast:192.168.122.255  Mask:255.255.255.0
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:0 (0.0 b)  TX bytes:0 (0.0 b)
$ netstat -nr
Destination     Gateway         Genmask         Flags   MSS Window  irtt Iface
172.16.22.11    0.0.0.0         255.255.255.255 UH        0 0          0 lo
172.16.22.0     0.0.0.0         255.255.255.0   U         0 0          0 eth0
25.25.107.0     0.0.0.0         255.255.255.0   U         0 0          0 eth1
192.168.122.0   0.0.0.0         255.255.255.0   U         0 0          0 virbr0
89.72.0.0       0.0.0.0         255.255.0.0     U         0 0          0 ib0
169.254.0.0     0.0.0.0         255.255.0.0     U         0 0          0 eth0
169.254.0.0     0.0.0.0         255.255.0.0     U         0 0          0 eth1
169.254.0.0     0.0.0.0         255.255.0.0     U         0 0          0 ib0
12.0.0.0        0.0.0.0         255.0.0.0       U         0 0          0 gn0
0.0.0.0         172.16.22.254   0.0.0.0         UG        0 0          0 eth0
$ lspci |grep -i  'network\|ethernet'
06:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)
06:00.1 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)
06:00.2 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)
06:00.3 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)
$ lspci |grep -i  'infini'
42:00.0 InfiniBand: Mellanox Technologies MT26428 [ConnectX VPI PCIe 2.0 5GT/s - IB QDR / 10GigE] (rev b0)
```

本地回环网卡已忽略。最后一个 virbr0 是本机虚拟机使用的网段，因为 192.168.122.0/24 是 virsh 默认建立虚拟机利用的网段，可以检查登陆节点确实安装了 virsh。 ib0 明显是使用 InfiniBand 的高速计算网络。总结一下上面出现有流量的网段包括 `89.72.0/16`, `12.0/8`, `172.16.22.0/24`。其中大部分连局域网预留网段都不是，不过整个系统内部和因特网不联通也就无所谓了。

要想了解这些网段的工作性质，还是要从 `/etc/hosts` 入手。其 hostname 的命名规则为 `machine-if`，前边一致对应的物理主机就是同一台。但一台物理机器的不同网卡自然可以横跨不同网段。具体信息如下。

* 25.8.2.0-25.8.2.15, login nodes, eth0
* 25.8.6.0-25.8.6.18, manage nodes,  eth0
* 12.11.70-12.11.71, manage nodes, gn0(default)
* 12.11.70-12.11.71, login nodes, gn0(default)
* 25.8.14, storage nodes, (default)
* 30.30, storage nodes, manage nodes and login nodes, IPMI
* 89.72.2 login nodes, ib0
* 89.72.14, storage nodes, ib0
* 12.0-12.11, computer nodes, (default)。 12.11 有500个左右的节点，同时是 io nodes (default).
* 89.72.100-89.72.101, io nodes, ib0

从上面可以看出，网络主要包括 12 的计算网，25.8 的管理网和 89.72 的 infiniband 高速 IO 网络。30.30 网段对应的 IPMI 是服务器独立于 OS 的远程控制板。此外 hosts 还有额外的一些网段和机器，这里就不列举了。由 hosts 可以看出，超算上的绝大多数计算节点事实上是没有配置 infiniband 高速互联的。因为高速互联的网段是 `89.72`，只有提供 Lustre 文件系统的集中存储节点，和极少部分对 IO 要求高的计算节点（标记为 ion）配置了高速互联网络。因此该网络这主要用来加速存储 IO，而非 mpi 型的计算通讯。这些计算通讯还是在 `12.0/8` 网段中解决的，而这一网段的硬件是 Intel I350，这只是一张千兆网卡，唯一的特别处理是这一网段有 Jumbo Frame 加成，MTU 高达 32750。

通过 hosts 和其他的信息，各种节点的数量分布大致如下。管理节点，10+；登陆节点，10+；存储节点，100+；计算节点，10000 左右。

计算节点网络拓扑较简单，需要注意计算节点的多样性，很多其他类型的计算节点没有开通使用权限，就没办法查看了，但根据 hosts 的内容，应该有 ion 系列的计算节点，是具有 InfiniBand 界面，可以高速直连存储节点的。

```bash
$ hostname
cn9886
$ ifconfig
eth0      Link encap:Ethernet  HWaddr 00:12:04:01:3F:28
          inet addr:10.0.0.1  Bcast:10.0.255.255  Mask:255.255.0.0
          UP BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:0 (0.0 b)  TX bytes:0 (0.0 b)
          Memory:dfda0000-dfdc0000

gn0       Link encap:Ethernet  HWaddr FC:FC:FC:1E:44:06
          inet addr:12.6.70.*  Bcast:12.255.255.255  Mask:255.0.0.0
          UP BROADCAST RUNNING  MTU:32750  Metric:1
          RX packets:362354675 errors:0 dropped:15 overruns:0 frame:0
          TX packets:381921321 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:398962189356 (371.5 GiB)  TX bytes:957872702399 (892.0 GiB)
$ netstat -nr
Destination     Gateway         Genmask         Flags   MSS Window  irtt Iface
10.0.0.0        0.0.0.0         255.255.0.0     U         0 0          0 eth0
12.0.0.0        0.0.0.0         252.0.0.0       U         0 0          0 gn0
0.0.0.0         10.0.1.254      0.0.0.0         UG        0 0          0 eth0
```

计算节点已忽略了没有 ip 地址的和 loopback 网卡。上述两张网卡，似乎只有 `gn0` 有流量，`eth0` 并无流量，注意到 eth0 这张网卡很特殊，因为所有计算节点的地址都是同样的 `10.0.0.1`。还有一个细节是 gn0 网卡上的 MTU，高达 32750，这说明计算网络里开启了 jumbo frame，可以传送很大的 packet，防止数据碎片。32750 已经是默认 MTU 1500 的 20 倍了。这数字还是很惊人的，因为一般的交换机硬件也就支持 9000 的 MTU。

### 软件设定

可以通过 `export` 来观察超算上默认的环境变量。其中 `HOME=/WORK/<username>`，也即用户的家目录，挂载在 Lustre 文件系统上，而非登陆节点的根路径下。` MODULEPATH="/WORK/app/modulefiles"`, `MODULESHOME="/usr/share/Modules"` 给出了 module file 的使用位置和 module 工具的位置。其中 `/WORK/app` 也是大部分应用的安装位置。此外还有很多和 intel 全家桶相关的环境变量，使得 icc 之类的在默认环境就可以直接使用，而无需 module 加载。

利用 `service --status-all` 可以查看登陆节点运行的后台服务。开启的服务并不多，事实上，大部分集群管理相关的配置应该都在普通用户无法访问的管理节点之上，这确保了整个系统的安全性。

通过扫描端口，可以看到管理节点向内网提供的服务。

```bash
$ nmap -v mn5
PORT     STATE SERVICE
22/tcp   open  ssh
111/tcp  open  rpcbind
389/tcp  open  ldap
2049/tcp open  nfs
3306/tcp open  mysql
4567/tcp open  tram
5666/tcp open  nrpe
5989/tcp open  wbem-https
8000/tcp open  http-alt
8649/tcp open  unknown
8651/tcp open  unknown
8652/tcp open  unknown
```

其中三个 unknown 应该是 slurmctld，slurmd 和 slurmdbd 监听的端口。nrpe 是 nagios 相关的服务。两个 http 相关的端口，curl 都无法获取到信息。大部分管理软件的详细情况，由于设置在普通用户无法登陆的管理节点，因此无法了解。

登陆节点也设置了 cgroup ，不过通过 `lscgroup` 观察，似乎主要是对于虚拟机 libvirt 的限制。


## Slurm 相关

### slurm 配置

主配置文件位于 `/etc/slurm/slurm.conf`，通过 `Include` 的方式，还包括了 sched.conf, node.conf, partition.conf 等处于同一文件夹下的配置文件。slurmctld 的主节点和备用节点分别是 mn5 和 mn7。`AccountingStorageHost` 同样也是 mn5 和 mn7。`PrivateData=job` 的设置，使得 `squeue` 时不会看到其他人的任务，但下文会提及，`sshare` 的其他用户信息并未能够隐藏。

在 sched.conf 中，设置了`SelectType=select/linear`，也即每个节点只能全部独占，而不能在不同任务间共享资源，这大幅降低了设置的难度，虽然可能会造成一定的资源浪费。同时该超算上没有开启 Preempt 这种抢占模式。

node.conf 中给出了节点的相关设置：

```bash
NodeName=DEFAULT State=UNKNOWN Sockets=2 CoresPerSocket=12 ThreadsPerCore=1 RealMemory=64000

NodeName=cn[4224-4351,4480-5759,6912-13823,17024-17535]
```

可以看出，所有计算节点都是双路 CPU，且都没有开启超线程。计算节点总数为:

```bash
$ yhcontrol show hostnames cn[4224-4351,4480-5759,6912-13823,17024-17535]|wc -l
8832
```

也即该超算可用的计算 CPU 核总数超过 20 万。

partition.conf 的配置比较复杂，分区较多。首先使用 RootOnly 的分区 all，用于日常运维。此外 cn[9600-10111] 为 backup 分区，也是 RootOnly 的。分区配置中，管理员都没有利用 slurm 的 account 概念来 allow，而是都使用了 AllowGroups 来通过 Unix 的用户组来管理 slurm 分区权限。cn[11648-11775] 是 GPU 分区。cn[16256-16287] 是 localdisk 分区，也即只有这些计算节点上才有本地硬盘。cn[11776-11903] 是 MEM_128 分区，也即这些节点具有 128G 内存，而非64G。此外还有大量其他分区，这里就不列举了。最普通的是 `Default=YES`的 work 分区，共有 3840 个计算节点。观察分区，还有些名字带 docker 的分区，不过不知道具体的配置和使用情景。

### slurm 账户

虽然 `slurm.conf` 中配置了 `PrivateData`， 但 `sshare` 仍可以看到其他用户的数据。具体数据和用户名不便公开，可以看些统计。

```bash
$ sshare|grep zju|wc -l
41
$ sshare|grep ustc|wc -l
25
$ sshare|grep tju|wc -l
10
$ sshare|grep 'thu\|tsinghua'|wc -l
39
$ sshare|grep sysu|wc -l
251
$ sshare -a|awk '{if (NF>6) print $0;}'|wc -l
4708
$ getent passwd|wc -l
7827
```

可以看到总用户数目是 4708 个，不过 `getent` 得到的数目则是 7000 多。普通用户甚至可以直接查看最近时间内，自己计算的资源在所有用户中的排位，命令为 `sshare -a|awk '{if (NF>6) print $0;}'|sort -n -r -k 5|cat -n|grep <username>`. 显示的第一列即为对应用户在近期使用量的排序。依据是 `RawUsage` 做的排序。

### 用户管理

Linux 账户管理，是使用了 LDAP 的形式，根据 `/etc/openldap/ldap.conf` 可以看出 ldap 的服务器在 mn5 和 mn7 两个管理节点。因此简单的 `cat /etc/passwd|wc -l` 可以发现用户数量很少，其全部是一些应用对应的独立用户。通过 `/etc/nsswitch.conf`，可以发现 `passwd: files ldap` 行，确证了用户账户是被 LDAP 服务管理的。输入 `who` 可以查看到最近也登陆这台登陆节点的用户名。


